# Mathematics of Deep Learning

http://rt.dgyblog.com/ref/ref-learning-deep-learning.html

https://github.com/leiwu1990/course.math_theory_nn

http://www.mit.edu/~9.520/fall18/

2018上海交通大学深度学习理论前沿研讨会 - 凌泽南的文章 - 知乎
https://zhuanlan.zhihu.com/p/40097048

https://www.researchgate.net/project/Theories-of-Deep-Learning

[A mathematical theory of deep networks and of why they work as well as they do is now emerging. I will review some recent theoretical results on the approximation power of deep networks including conditions under which they can be exponentially better than shallow learning. A class of deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. I will also discuss another puzzle around deep networks: what guarantees that they generalize and they do not overfit despite the number of weights being larger than the number of training data and despite the absence of explicit regularization in the optimization?](http://www.mit.edu/~9.520/fall17/Classes/deep_learning_theory.html)

Deep Neural Networks and Partial Differential Equations: Approximation Theory and
Structural Properties
Philipp Petersen, University of Oxford 

https://memento.epfl.ch/event/a-theoretical-analysis-of-machine-learning-and-par/

- http://at.yorku.ca/c/b/p/g/30.htm
- https://mat.univie.ac.at/~grohs/
- [Topics course Mathematics of Deep Learning, NYU, Spring 18](https://joanbruna.github.io/MathsDL-spring18/)
- https://skymind.ai/ebook/Skymind_The_Math_Behind_Neural_Networks.pdf
- https://github.com/markovmodel/deeptime
- https://omar-florez.github.io/scratch_mlp/
- https://joanbruna.github.io/MathsDL-spring19/
- https://github.com/isikdogan/deep_learning_tutorials
- https://www.brown.edu/research/projects/crunch/machine-learning-x-seminars
- [Deep Learning: Theory & Practice](http://anotherdatum.com/tce_2018.html)
- https://www.math.ias.edu/wtdl
- https://www.ml.tu-berlin.de/menue/mitglieder/klaus-robert_mueller/
- https://www-m15.ma.tum.de/Allgemeines/MathFounNN
- https://www.math.purdue.edu/~buzzard/MA598-Spring2019/index.shtml
- http://mathematics-in-europe.eu/?p=801
- [Discrete Mathematics of Neural Networks: Selected Topics](https://epubs.siam.org/doi/book/10.1137/1.9780898718539?mobileUi=0)
- https://cims.nyu.edu/~bruna/
- https://www.math.ias.edu/wtdl
- https://www.pims.math.ca/scientific-event/190722-pcssdlcm
- [Deep Learning for Image Analysis EMBL COURSE](https://www.embl.de/training/events/2020/MAC20-01/)
- http://voigtlaender.xyz/
- http://www.mit.edu/~9.520/fall19/

[angewandtefunktionalanalysis]

(https://www.math.tu-berlin.de/fachgebiete_ag_modnumdiff/angewandtefunktionalanalysis/v_menue/mitarbeiter/kutyniok/v_menue/kutyniok_publications/)


## Numerical Analysis for Deep Learning

<img src="http://www.mathcs.emory.edu/~lruthot/img/DeepLearning.png" width="80%" />

- https://arxiv.org/pdf/1904.05657.pdf
- [CS 584 / MATH 789R - Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/math789r-sp20.html)
- http://www.mathcs.emory.edu/~lruthot/teaching.html
- [Numerical methods for deep learning](https://github.com/IPAIopen/NumDL-CourseNotes)
- [Short Course on Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/index.html)
- https://www.math.ucla.edu/applied/cam
- http://www.mathcs.emory.edu/~lruthot/
- [Automatic Differentiation of Parallelised Convolutional
Neural Networks - Lessons from Adjoint PDE Solvers](https://autodiff-workshop.github.io/slides/Hueckelheim_nips_autodiff_CNN_PDE.pdf)
- https://raoyongming.github.io/
- [MA 721: Topics in Numerical Analysis: Deep Learning](http://www.ms.uky.edu/~qye/MA721/ma721F17.html)


## Dynamics for Deep Learning

- https://web.stanford.edu/~yplu/DynamicOCNN.pdf
- https://zhuanlan.zhihu.com/p/71747175
- https://web.stanford.edu/~yplu/
- https://web.stanford.edu/~yplu/project.html
- [A Flexible Optimal Control Framework for Efficient Training of Deep Neural Networks](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1751636)
- [NEURAL NETWORKS AS ORDINARY DIFFERENTIAL EQUATIONS](https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/)
- [BRIDGING DEEP NEURAL NETWORKS AND DIFFERENTIAL EQUATIONS FOR IMAGE ANALYSIS AND BEYOND](http://helper.ipam.ucla.edu/publications/glws3/glws3_15460.pdf)
- [Deep learning for universal linear embeddings of nonlinear dynamics](https://doaj.org/article/9d9172e9bf324cc6ac6d48ff8e234a85)
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](http://ganguli-gang.stanford.edu/pdf/DynamLearn.pdf)
- [Neural Ordinary Differential Equations](http://www.cs.toronto.edu/~rtqichen/pdfs/neural_ode_slides.pdf)

<img src="https://rkevingibson.github.io/img/ode_networks_1.png" width="80%" />

+ [NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data](https://www.arxiv-vanity.com/papers/1908.03190/)
+ [Neural Ordinary Differential Equations and Adversarial Attacks](https://rajatvd.github.io/Neural-ODE-Adversarial/)
+ [Neural Dynamics and Computation Lab](http://ganguli-gang.stanford.edu/)

## Approximation Theory for Deep Learning

Universal approximation theory show the expression power of deep neural network of some wide while shallow neural network.


- https://deeplearning-math.github.io/
- [Deep Neural Network Approximation Theory](https://arxiv.org/abs/1901.02220)
- [Provable approximation properties for deep neural networks](https://cpsc.yale.edu/sites/default/files/files/tr1513(1).pdf)
- [Deep Learning: Approximation of Functions by Composition](http://helper.ipam.ucla.edu/publications/dlt2018/dlt2018_14936.pdf)
- [DGD Approximation Theory Workshop](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Petersen/DGD_Approximation_Theory.pdf)

## Differential Equation and Deep Learning

[We derive upper bounds on the complexity of ReLU neural networks approximating the solution maps of parametric partial differential equations. In particular, without any knowledge of its concrete shape, we use the inherent low-dimensionality of the solution manifold to obtain approximation rates which are significantly superior to those provided by classical approximation results. We use this low dimensionality to guarantee the existence of a reduced basis. Then, for a large variety of parametric partial differential equations, we construct neural networks that yield approximations of the parametric maps not suffering from a curse of dimension and essentially only depending on the size of the reduced basis.](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/Parametric_PDEs_and_NNs_.pdf)

- https://arxiv.org/abs/1804.04272
- https://deepai.org/machine-learning/researcher/weinan-e
- https://deepxde.readthedocs.io/en/latest/
- https://github.com/IBM/pde-deep-learning
- https://github.com/ZichaoLong/PDE-Net
- https://github.com/amkatrutsa/DeepPDE
- https://github.com/maziarraissi/DeepHPMs
- https://maziarraissi.github.io/DeepHPMs/
- [DGM: A deep learning algorithm for solving partial differential equations](https://www.sciencedirect.com/science/article/pii/S0021999118305527)
- [A Theoretical Analysis of Deep Neural Networks and Parametric PDEs](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/Parametric_PDEs_and_NNs_.pdf)
- [NeuralNetDiffEq.jl: A Neural Network solver for ODEs](https://julialang.org/blog/2017/10/gsoc-NeuralNetDiffEq)

https://arxiv.org/abs/1806.07366
https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/

## Inverse Problem and Deep Learning

[There is a long history of algorithmic development for solving inverse problems arising in sensing and imaging systems and beyond. Examples include medical and computational imaging, compressive sensing, as well as community detection in networks. Until recently, most algorithms for solving inverse problems in the imaging and network sciences were based on static signal models derived from physics or intuition, such as wavelets or sparse representations.](https://deep-inverse.org/)

[Today, the best performing approaches for the aforementioned image reconstruction and sensing problems are based on deep learning, which learn various elements of the method including i) signal representations, ii) stepsizes and parameters of iterative algorithms, iii) regularizers, and iv) entire inverse functions. For example, it has recently been shown that solving a variety of inverse problems by transforming an iterative, physics-based algorithm into a deep network whose parameters can be learned from training data, offers faster convergence and/or a better quality solution. Moreover, even with very little or no learning, deep neural networks enable superior performance for classical linear inverse problems such as denoising and compressive sensing. Motivated by those success stories, researchers are redesigning traditional imaging and sensing systems.](https://deep-inverse.org/)

- http://cpaior2019.uowm.gr/
- https://github.com/tankconcordia/deep_inv_opt
- https://amds123.github.io/2019/01/13/Neumann-Networks-for-Inverse-Problems-in-Imaging/
- https://github.com/mughanibu/Deep-Learning-for-Inverse-Problems
- https://cv.snu.ac.kr/research/VDSR/
- https://arxiv.org/abs/1803.00092
- https://deep-inverse.org/
- https://earthscience.rice.edu/mathx2019/
- [Deep Learning and Inverse Problem](https://www.dlip.org/)
- https://www.scec.org/publication/8768

## Random Matrix Theory and Deep Learning

Random matrix focus on the matrix, whose entities are sampled from  some specific probability distribution.
Weight matrices in deep nerual network are initialed in random. 
However, the model is over-parametered and it is hard to verify the role of one individual parameter.


- [Recent Advances in Random Matrix Theory for Modern Machine Learning](https://zhenyu-liao.github.io/pdf/pre/Matrix_talk_liao_handout.pdf)
- http://romaincouillet.hebfree.org/
- https://zhenyu-liao.github.io/
- https://dionisos.wp.imt.fr/
- [Features extraction using random matrix theory](https://ir.library.louisville.edu/cgi/viewcontent.cgi?article=2227&context=etd)
- [Nonlinear random matrix theory for deep learning](https://papers.nips.cc/paper/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf)
- [A RANDOM MATRIX APPROACH TO NEURAL NETWORKS](https://arxiv.org/pdf/1702.05419.pdf)
- [Tensor Programs: A Swiss-Army Knife for Nonlinear Random Matrix Theory of Deep Learning and Beyond](https://www.csail.mit.edu/event/tensor-programs-swiss-army-knife-nonlinear-random-matrix-theory-deep-learning-and-beyond)
- [Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation](https://arxiv.org/abs/1902.04760)

http://www.vision.jhu.edu/tutorials/CVPR16-Tutorial-Math-Deep-Learning-Raja.pdf

## Deep learning and Optimal Transport

[Optimal transport (OT) provides a powerful and flexible way to compare probability measures, of all shapes: absolutely continuous, degenerate, or discrete. This includes of course point clouds, histograms of features, and more generally datasets, parametric densities or generative models. Originally proposed by Monge in the eighteenth century, this theory later led to Nobel Prizes for Koopmans and Kantorovich as well as Villani’s Fields Medal in 2010.](http://otml17.marcocuturi.net/)

- http://otml17.marcocuturi.net/
- https://www-obelix.irisa.fr/files/2017/01/postdoc-Obelix.pdf
- http://www.cis.jhu.edu/~rvidal/talks/learning/StructuredFactorizations.pdf
- http://cmsa.fas.harvard.edu/wp-content/uploads/2018/06/David_Gu_Harvard.pdf
- https://mc.ai/optimal-transport-theory-the-new-math-for-deep-learning/
- https://www.louisbachelier.org/wp-content/uploads/2017/07/170620-ilb-presentation-gabriel-peyre.pdf
- http://people.csail.mit.edu/davidam/
- https://www.birs.ca/events/2020/5-day-workshops/20w5126
- https://github.com/hindupuravinash/nips2017

## Geometric Analysis Approach to AI

- http://cmsa.fas.harvard.edu/geometric-analysis-ai/
- http://inspirehep.net/record/1697651
- https://diglib.eg.org/handle/10.2312/2631996
- http://ubee.enseeiht.fr/skelneton/
- https://biomedicalimaging.org/2019/tutorials/
